{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67da5314",
   "metadata": {},
   "outputs": [],
   "source": [
    "from segpy.reader import create_reader\n",
    "import numpy as np\n",
    "import os\n",
    "import pywt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0cda7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "directory = r'C:\\Users\\user\\Desktop\\2D\\Correlated_Shot_Gathers'\n",
    "\n",
    "for file_name in os.listdir(directory):\n",
    "    \n",
    "    if file_name != \".segpy\":\n",
    "        \n",
    "        with open(directory + \"\\\\\" + file_name, 'rb') as segy_in_file:\n",
    "\n",
    "            seg_y_dataset = create_reader(segy_in_file, endian='<')\n",
    "\n",
    "            for i in seg_y_dataset.trace_indexes():\n",
    "                dataset.append(seg_y_dataset.trace_samples(i))\n",
    "            \n",
    "dataset = np.array(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "def4bf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_dataset = np.max(dataset, axis = 1)\n",
    "min_dataset = np.min(dataset, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8001e087",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = (dataset - np.repeat(np.expand_dims(min_dataset,axis = 1), 4001, axis = 1))/(np.repeat(np.expand_dims(max_dataset,axis = 1), 4001, axis = 1) - np.repeat(np.expand_dims(min_dataset,axis = 1), 4001, axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f66028c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def quantize(signal, levels):\n",
    "    \n",
    "    kmeans = KMeans(n_clusters = levels).fit(signal.reshape(-1,1))\n",
    "    \n",
    "    return kmeans.labels_, kmeans.cluster_centers_.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10f6ad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(x,x_pred):\n",
    "    nmse = np.sum(np.square(x - x_pred))/np.sum(np.square(x))\n",
    "    nrmse = np.sqrt(np.mean(np.square(x - x_pred)))/(np.amax(x) - np.amin(x))\n",
    "    snr = -10*np.log10(nmse)\n",
    "    return nmse, nrmse, snr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8df7423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probability_dictionary(symbols, levels):\n",
    "\n",
    "    return {i:np.mean(symbols == i) for i in np.arange(levels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a7e9e640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_huffman_codes(symbol_probabilities):\n",
    "    \n",
    "    huffman_code = {str(key):'' for key in symbol_probabilities.keys()}\n",
    "    symbol_probabilities = {str(key):symbol_probabilities[key] for key in symbol_probabilities.keys()}\n",
    "    \n",
    "    while(len(symbol_probabilities) != 1):\n",
    "        \n",
    "        key1 = min(symbol_probabilities, key=symbol_probabilities.get)\n",
    "        value1 = symbol_probabilities[key1]\n",
    "        symbol_probabilities.pop(key1)\n",
    "        \n",
    "        key2 = min(symbol_probabilities, key=symbol_probabilities.get)\n",
    "        value2 = symbol_probabilities[key2]\n",
    "        symbol_probabilities.pop(key2)\n",
    "        \n",
    "        for symbol in key1.split('-'):\n",
    "            huffman_code[symbol] += '1'\n",
    "            \n",
    "        for symbol in key2.split('-'):\n",
    "            huffman_code[symbol] += '0'\n",
    "            \n",
    "        symbol_probabilities[key1 + '-' + key2] = value1 + value2\n",
    "            \n",
    "    return {int(key):huffman_code[key][::-1] for key in huffman_code.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "05181652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_huffman_encoeded_length(d1_sym, M1, d2_sym, M2, d3_sym, M3, c3_sym):\n",
    "    \n",
    "    length = 0\n",
    "    \n",
    "    d1_sym_huffman_dict = get_huffman_codes(get_probability_dictionary(d1_sym, M1))\n",
    "    \n",
    "    for key, value in d1_sym_huffman_dict.items():\n",
    "        length += np.sum(d1_sym == key)*len(value) \n",
    "        \n",
    "    d2_sym_huffman_dict = get_huffman_codes(get_probability_dictionary(d2_sym, M2))\n",
    "    \n",
    "    for key, value in d2_sym_huffman_dict.items():\n",
    "        length += np.sum(d2_sym == key)*len(value) \n",
    "        \n",
    "    d3_sym_huffman_dict = get_huffman_codes(get_probability_dictionary(d3_sym, M3))\n",
    "    \n",
    "    for key, value in d3_sym_huffman_dict.items():\n",
    "        length += np.sum(d3_sym == key)*len(value)\n",
    "        \n",
    "    c3_sym_huffman_dict = get_huffman_codes(get_probability_dictionary(c3_sym, M3))\n",
    "    \n",
    "    for key, value in c3_sym_huffman_dict.items():\n",
    "        length += np.sum(c3_sym == key)*len(value)\n",
    "        \n",
    "    return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9d65e938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cr_and_metrics(dataset, wavelet = 'db2', M1 = 4, M2 = 8, M3 = 16):\n",
    "    \n",
    "    nmse, nrmse, snr = np.zeros(len(dataset)), np.zeros(len(dataset)), np.zeros(len(dataset))\n",
    "    \n",
    "    total_length = 0\n",
    "    \n",
    "    for j, sample in enumerate(dataset):\n",
    "        \n",
    "        c3, d3, d2, d1 = pywt.wavedec(sample, wavelet, level = 3)\n",
    "        \n",
    "        d1_sym, d1_val = quantize(d1, M1)\n",
    "        d2_sym, d2_val = quantize(d2, M2)\n",
    "        d3_sym, d3_val = quantize(d3, M3)\n",
    "        c3_sym, c3_val = quantize(c3, M3)\n",
    "        \n",
    "        d1_reconstruct = np.array([d1_val[i] for i in d1_sym])\n",
    "        d2_reconstruct = np.array([d2_val[i] for i in d2_sym])\n",
    "        d3_reconstruct = np.array([d3_val[i] for i in d3_sym])\n",
    "        c3_reconstruct = np.array([c3_val[i] for i in c3_sym])\n",
    "        \n",
    "        reconstructed_signal = pywt.waverec((c3_reconstruct,d3_reconstruct,d2_reconstruct,d1_reconstruct), wavelet)\n",
    "        \n",
    "        nmse[j], nrmse[j], snr[j] = get_metrics(sample, reconstructed_signal[:-1])\n",
    "        \n",
    "        total_length += get_huffman_encoeded_length(d1_sym, M1, d2_sym, M2, d3_sym, M3, c3_sym)\n",
    "        \n",
    "    return 4001*64*len(dataset)/total_length, np.mean(nmse), np.mean(nrmse), np.mean(snr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9dbf0a60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22.639674423827252,\n",
       " 8.632897820461949e-06,\n",
       " 0.0013707870806683787,\n",
       " 52.21920866966248)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_cr_and_metrics(dataset[900:1000], 'db2', 8, 16, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd94fa40",
   "metadata": {},
   "source": [
    "Making a model that ignores d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fe84156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_huffman_encoeded_length_without_d1(d2_sym, M2, d3_sym, M3, c3_sym):\n",
    "    \n",
    "    length = 0\n",
    "        \n",
    "    d2_sym_huffman_dict = get_huffman_codes(get_probability_dictionary(d2_sym, M2))\n",
    "    \n",
    "    for key, value in d2_sym_huffman_dict.items():\n",
    "        length += np.sum(d2_sym == key)*len(value) \n",
    "        \n",
    "    d3_sym_huffman_dict = get_huffman_codes(get_probability_dictionary(d3_sym, M3))\n",
    "    \n",
    "    for key, value in d3_sym_huffman_dict.items():\n",
    "        length += np.sum(d3_sym == key)*len(value)\n",
    "        \n",
    "    c3_sym_huffman_dict = get_huffman_codes(get_probability_dictionary(c3_sym, M3))\n",
    "    \n",
    "    for key, value in c3_sym_huffman_dict.items():\n",
    "        length += np.sum(c3_sym == key)*len(value)\n",
    "        \n",
    "    return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e5e7f83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cr_and_metrics_without_d1(dataset, wavelet = 'db2', M2 = 8, M3 = 16):\n",
    "    \n",
    "    nmse, nrmse, snr = np.zeros(len(dataset)), np.zeros(len(dataset)), np.zeros(len(dataset))\n",
    "    \n",
    "    total_length = 0\n",
    "    \n",
    "    for j, sample in enumerate(dataset):\n",
    "        \n",
    "        c3, d3, d2, d1 = pywt.wavedec(sample, wavelet, level = 3)\n",
    "        d2_sym, d2_val = quantize(d2, M2)\n",
    "        d3_sym, d3_val = quantize(d3, M3)\n",
    "        c3_sym, c3_val = quantize(c3, M3)\n",
    "        \n",
    "        d2_reconstruct = np.array([d2_val[i] for i in d2_sym])\n",
    "        d3_reconstruct = np.array([d3_val[i] for i in d3_sym])\n",
    "        c3_reconstruct = np.array([c3_val[i] for i in c3_sym])\n",
    "        \n",
    "        reconstructed_signal = pywt.waverec((c3_reconstruct,d3_reconstruct,d2_reconstruct,np.zeros(2002)), wavelet)\n",
    "        \n",
    "        nmse[j], nrmse[j], snr[j] = get_metrics(sample, reconstructed_signal[:-1])\n",
    "        \n",
    "        total_length += get_huffman_encoeded_length_without_d1(d2_sym, M2, d3_sym, M3, c3_sym)\n",
    "        \n",
    "    return 4001*64*len(dataset)/total_length, np.mean(nmse), np.mean(nrmse), np.mean(snr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "41b66636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127.64905284147558,\n",
       " 0.014091713596524556,\n",
       " 0.05693899028792911,\n",
       " 19.56495417418365)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_cr_and_metrics_without_d1(dataset[900:1000], 'db2', 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "508d2495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107.62383103919302,\n",
       " 0.004737264925443791,\n",
       " 0.03347772928172762,\n",
       " 24.080715597525792)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_cr_and_metrics_without_d1(dataset[900:1000], 'db2', 2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f0ba0161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79.27948010613365,\n",
       " 0.0011798831569598011,\n",
       " 0.016570604862909784,\n",
       " 30.297794314585907)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_cr_and_metrics_without_d1(dataset[900:1000], 'db2', 4,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "16c550ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61.128882862408446,\n",
       " 0.0002684953694337118,\n",
       " 0.00784826569433506,\n",
       " 36.82138096645946)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_cr_and_metrics_without_d1(dataset[900:1000], 'db2', 4, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "adbb28c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44.22025161251327,\n",
       " 5.62403323986683e-05,\n",
       " 0.0035866707405213982,\n",
       " 43.60017020627727)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_cr_and_metrics_without_d1(dataset[900:1000], 'db2', 8, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ae1439a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32.63462560489601,\n",
       " 1.2989737528696998e-05,\n",
       " 0.0017683824279625423,\n",
       " 49.50535980326118)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_cr_and_metrics_without_d1(dataset[900:1000], 'db2', 16, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8242356",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
